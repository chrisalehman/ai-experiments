{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "mc32N7A6pagR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Llama 3.2 for customer service chat\n",
        "\n",
        "**Introduction**: Customer service chat is a common use case for LLMs. The purpose of this notebook is to determine how well the Llama 3.2-3B-Instruct base model can be fine-tuned to adopt the style of a helpful customer service bot.\n",
        "\n",
        "**Findings**: The model was fine-tuned on 3000 examples from the Bitext customer service training [dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset). As a result, it adopted the style of a customer service bot and provided relevant responses.\n",
        "\n",
        "**Summary of steps**:\n",
        "*   Load dataset, model and tokenizer\n",
        "*   Fine-tune QLoRA adapters\n",
        "*   Perform manual test to see bot adopting customer service style\n",
        "*   Merge with foundational model\n",
        "*   Convert to GGUF and quantize to facilitate its use locally in the Jan application\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xM75wGH0NazV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: libraries, dependencies, configurations"
      ],
      "metadata": {
        "id": "mc32N7A6pagR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load libraries and modules from Huggingface, Weights and Biases, and Google (for accessing Drive)."
      ],
      "metadata": {
        "id": "I_5vuvz4tzUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install -U transformers\n",
        "%pip install -U datasets\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U wandb\n",
        "\n",
        "from transformers import (\n",
        "  AutoModelForCausalLM,\n",
        "  AutoTokenizer,\n",
        "  BitsAndBytesConfig,\n",
        "  HfArgumentParser,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  logging,\n",
        ")\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModel,\n",
        "  prepare_model_for_kbit_training,\n",
        "  get_peft_model,\n",
        ")\n",
        "import os, torch, wandb, pprint\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi, login as hf_login"
      ],
      "metadata": {
        "id": "ACO26ToRpdX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive, connect to HF and W&B. Initialize configuration."
      ],
      "metadata": {
        "id": "RhyZ7MUksbFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# connect to huggingface\n",
        "hf_auth_token = userdata.get('HF_TOKEN')\n",
        "hf_login(hf_auth_token)\n",
        "\n",
        "# connect to weights and biases\n",
        "wb_auth_token = userdata.get('WB_TOKEN')\n",
        "wandb.login(key=wb_auth_token)\n",
        "\n",
        "# initialize wandb.ai with this project\n",
        "run = wandb.init(\n",
        "  project='Fine-tune Llama 3.2 3B Instruct on Customer Service Dataset',\n",
        "  job_type=\"training\",\n",
        "  anonymous=\"allow\"\n",
        ")\n",
        "\n",
        "# base model config\n",
        "base_model_namespace = \"meta-llama\"\n",
        "base_model = \"Llama-3.2-3B-Instruct\"\n",
        "base_model_name = f\"{base_model_namespace}/{base_model}\"\n",
        "\n",
        "# base model cache config\n",
        "base_model_cache_base_directory = \"/content/drive/MyDrive/.model_cache\"\n",
        "base_model_provider = \"huggingface\"\n",
        "base_model_cache_directory = f\"{base_model_cache_base_directory}/{base_model_provider}/{base_model_name}\"\n",
        "\n",
        "# base dataset cache config\n",
        "base_dataset_cache_base_directory = \"/content/drive/MyDrive/.dataset_cache\"\n",
        "base_dataset_provider = \"huggingface\"\n",
        "base_dataset_namespace = \"bitext\"\n",
        "base_dataset = \"Bitext-customer-support-llm-chatbot-training-dataset\"\n",
        "base_dataset_name = f\"{base_dataset_namespace}/{base_dataset}\"\n",
        "base_dataset_cache_directory = f\"{base_dataset_cache_base_directory}/{base_dataset_provider}/{base_dataset_name}\"\n",
        "\n",
        "# project config\n",
        "custom_model_name = \"llama-3.2-3B-customer-service-chatbot\"\n",
        "model_directory = f\"{base_model_cache_base_directory}/model-llama-3.2-3b-instruct\"\n",
        "\n",
        "# mixed precision datatypes\n",
        "torch_dtype = torch.float16\n",
        "use_fp16 = True\n",
        "use_bf16 = False"
      ],
      "metadata": {
        "id": "5-Ebjastpwmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Load and configure model, prepare dataset"
      ],
      "metadata": {
        "id": "7YrOLsvTvEZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and tokenizer. Load memory-optimized version of the base model where weigths stored in 4-bit quantized format, but computations in bfloat16 or float16, depending on the hardware. These optimizations will speed up training.\n",
        "\n",
        "NOTE: I'm getting a package conflict error relating to flash attention 2, so disabling for now."
      ],
      "metadata": {
        "id": "lS8EuWL1uNqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set flash attention mechanism\n",
        "attn_implementation = \"eager\"\n",
        "\n",
        "# quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True, # 4-bit quantized version\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch_dtype,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  base_model_name,\n",
        "  quantization_config=bnb_config,\n",
        "  device_map=\"auto\",\n",
        "  attn_implementation=attn_implementation\n",
        ")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "\n",
        "# check to see which dtype and attention mechanism is possible\n",
        "print(torch_dtype)\n",
        "print(attn_implementation)"
      ],
      "metadata": {
        "id": "Eo1Qk8DdsKF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the dataset. Load, shuffle, split, and select sample size. The dataset contains 26.9k samples.\n",
        "\n",
        "We'll set a small sample size of 1000 for demonstration purposes."
      ],
      "metadata": {
        "id": "EJI6ZxwawZzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 3000\n",
        "dataset = load_dataset(base_dataset_name, split=\"train\")\n",
        "dataset = dataset.shuffle(seed=65).select(range(sample_size))\n",
        "\n",
        "instruction = \"\"\"\n",
        "You are a top-rated customer service agent named John.\n",
        "Be polite to customers and answer all their questions.\n",
        "\"\"\"\n",
        "\n",
        "def format_chat_template(row):\n",
        "  row_json = [{\"role\": \"system\", \"content\": instruction },\n",
        "              {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
        "              {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
        "\n",
        "  row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "  return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "  format_chat_template,\n",
        "  num_proc= 4,\n",
        ")\n",
        "\n",
        "# output an example\n",
        "pprint.pprint(dataset['text'][0])\n",
        "\n",
        "# split into train (90%) and test (10%)\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "OW_C0br2ve9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the trainable modules in the model. We'll use a custom function to extract the modules; then we'll apply them to the LoRA config."
      ],
      "metadata": {
        "id": "AuE6GmLIFKX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "def find_all_module_names(model):\n",
        "\n",
        "  cls = bnb.nn.Linear4bit\n",
        "  lora_module_names = set()\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, cls):\n",
        "      names = name.split('.')\n",
        "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "  if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
        "    lora_module_names.remove('lm_head')\n",
        "\n",
        "  return list(lora_module_names)\n",
        "\n",
        "modules = find_all_module_names(model)\n",
        "print(modules)"
      ],
      "metadata": {
        "id": "KUKl7zW9EVVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "  r=16,\n",
        "  lora_alpha=32,\n",
        "  lora_dropout=0.05,\n",
        "  bias=\"none\",\n",
        "  task_type=\"CAUSAL_LM\",\n",
        "  target_modules=modules\n",
        ")\n",
        "\n",
        "# clear out pre-existing template to avoid error when replacing it\n",
        "tokenizer.chat_template = None\n",
        "\n",
        "# replace chat template (function from trl package)\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "print(tokenizer.chat_template)\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "RsCo32wiGDTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the hyperparameters and training parameters. I'm using an Nvidia A100, so I can support slightly larger batch sizes and bfloat16 mixed precision for faster training."
      ],
      "metadata": {
        "id": "uR9T3qiZIYDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparamters\n",
        "training_arguments = SFTConfig(\n",
        "  output_dir=model_directory,\n",
        "  per_device_train_batch_size=4,\n",
        "  per_device_eval_batch_size=4,\n",
        "  gradient_accumulation_steps=1,\n",
        "  optim=\"paged_adamw_32bit\",\n",
        "  num_train_epochs=3,\n",
        "  eval_strategy=\"steps\",\n",
        "  eval_steps=0.1, # evaluate every 1/10 increment through the run\n",
        "  logging_steps=50,\n",
        "  warmup_steps=50,\n",
        "  logging_strategy=\"steps\",\n",
        "  learning_rate=2e-4,\n",
        "  fp16=False,\n",
        "  bf16=True,\n",
        "  group_by_length=True,\n",
        "  report_to=\"wandb\",\n",
        "  max_seq_length=512, # avoids exceeding GPU memory during training\n",
        "  dataset_text_field=\"text\",\n",
        "  packing= False,\n",
        ")\n",
        "\n",
        "# setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "  model=model,\n",
        "  train_dataset=dataset[\"train\"],\n",
        "  eval_dataset=dataset[\"test\"],\n",
        "  peft_config=peft_config,\n",
        "  processing_class=tokenizer,\n",
        "  args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "XD5ru22kGh0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train, validate, and save model"
      ],
      "metadata": {
        "id": "x4oQ_iIhcWpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start training."
      ],
      "metadata": {
        "id": "STbiMfkeOsb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start training\n",
        "trainer.train()\n",
        "\n",
        "# save results locally to view\n",
        "wandb.finish()\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "--W1BpaQN5bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model manually."
      ],
      "metadata": {
        "id": "UKWvuI2mcd71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_test(model, tokenizer):\n",
        "\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": instruction},\n",
        "    {\"role\": \"user\", \"content\": \"I bought the same item twice. Can I cancel order {{Order Number}}?\"}]\n",
        "\n",
        "  # set chat template\n",
        "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "  # tokenize the prompt\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  # generate a response\n",
        "  outputs = model.generate(**inputs, max_new_tokens=512, num_return_sequences=1)\n",
        "\n",
        "  # decodes back to text and outputs\n",
        "  text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return text.split(\"assistant\")[1]\n",
        "\n",
        "print(execute_test(model, tokenizer))"
      ],
      "metadata": {
        "id": "R7x83c58XZaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the LoRA adapter locally and push it to huggingface."
      ],
      "metadata": {
        "id": "3yiQ8mERdyGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the adapter locally\n",
        "trainer.model.save_pretrained(model_directory)\n",
        "tokenizer.save_pretrained(model_directory)\n",
        "\n",
        "# push to huggingface\n",
        "trainer.model.push_to_hub(\n",
        "  custom_model_name,\n",
        "  repo_type=\"model\",\n",
        "  use_temp_dir=True,\n",
        "  token=hf_auth_token)"
      ],
      "metadata": {
        "id": "m0gBPul4dE-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge and export fine-tuned model"
      ],
      "metadata": {
        "id": "TwDj2lUT8hhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, reload the base model and tokenizer. Then merge the adapter with the base model."
      ],
      "metadata": {
        "id": "hM654ggC-TGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "\n",
        "# reload base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  base_model_name,\n",
        "  low_cpu_mem_usage=True,\n",
        "  return_dict=True,\n",
        "  torch_dtype=torch_dtype,\n",
        "  device_map='auto',\n",
        ")\n",
        "\n",
        "# clear out pre-existing template to avoid error when replacing it\n",
        "tokenizer.chat_template = None\n",
        "\n",
        "# replace chat template (function from trl package)\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "print(tokenizer.chat_template)\n",
        "\n",
        "# merge model\n",
        "merged_model = PeftModel.from_pretrained(model, model_directory)\n",
        "merged_model = merged_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "Ca-3hpyteIZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check test\n",
        "print(execute_test(merged_model, tokenizer))"
      ],
      "metadata": {
        "id": "koytRzL5_kyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save merged model locally\n",
        "merged_model.save_pretrained(model_directory)\n",
        "tokenizer.save_pretrained(model_directory)\n",
        "\n",
        "# push the merged model to huggingface hub\n",
        "merged_model.push_to_hub(custom_model_name, use_temp_dir=True, token=hf_auth_token)\n",
        "tokenizer.push_to_hub(custom_model_name, use_temp_dir=True, token=hf_auth_token)"
      ],
      "metadata": {
        "id": "FeZaBfzWBCQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the merged model to GGUF and quantize"
      ],
      "metadata": {
        "id": "NBMT8ijnConn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_dir_base = f\"{model_directory}/.tmp\"\n",
        "tmp_dir = f\"{tmp_dir_base}/llama.cpp\"\n",
        "\n",
        "# set working directory\n",
        "if not os.path.exists(f\"{tmp_dir_base}\"):\n",
        "  os.mkdir(f\"{tmp_dir_base}\")\n",
        "os.chdir(f\"{tmp_dir_base}\")\n",
        "\n",
        "# clone llama.ccp repo\n",
        "if not os.path.exists(f\"{tmp_dir}\"):\n",
        "  !git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n",
        "\n",
        "# change to local repo\n",
        "os.chdir(f\"{tmp_dir}\")\n",
        "\n",
        "# build llama.ccp with 8 parallel processes to speed it up\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release -j 8\n",
        "\n",
        "# convert safetensors model format to gguf\n",
        "!python convert_hf_to_gguf.py \"/content/drive/MyDrive/Experiments/Fine-tuning/llama-3.2-3B-customer-service-chatbot\" \\\n",
        "  --outfile \"/content/drive/MyDrive/Experiments/Fine-tuning/llama-3.2-3B-customer-service-chatbot/model.gguf\" \\\n",
        "  --outtype f16\n",
        "\n",
        "# now quantize the model, reducing size from 16GB to ~4.6GB\n",
        "!./build/bin/llama-quantize \\\n",
        "  \"/content/drive/MyDrive/Experiments/Fine-tuning/llama-3.2-3B-customer-service-chatbot/model.gguf\" \\\n",
        "  \"/content/drive/MyDrive/Experiments/Fine-tuning/llama-3.2-3B-customer-service-chatbot/model-Q4_K_M.gguf\" \\\n",
        "  \"Q4_K_M\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "BfivkFj6BrzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# push the the gguf and quantized gguf models to huggingface\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# upload gguf\n",
        "api.upload_file(\n",
        "  path_or_fileobj=f\"{model_directory}/model.gguf\",\n",
        "  path_in_repo=\"model.gguf\",\n",
        "  repo_id=f\"chrisalehman/{custom_model_name}\",\n",
        "  repo_type=\"model\",\n",
        ")\n",
        "\n",
        "# upload quantized gguf\n",
        "api.upload_file(\n",
        "  path_or_fileobj=f\"{model_directory}/model-Q4_K_M.gguf\",\n",
        "  path_in_repo=\"model-Q4_K_M.gguf\",\n",
        "  repo_id=f\"chrisalehman/{custom_model_name}\",\n",
        "  repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "id": "IMRnY6lpDnyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xivmkKwtF-aN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}