{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2301f00d",
      "metadata": {
        "id": "2301f00d"
      },
      "source": [
        "# Vertex AI LLM Deployment Demonstration\n",
        "This notebook walks through an end-to-end example of fine-tuning a GPT-like model, saving it, uploading to Google Cloud Storage (GCS), building a custom Docker container, and deploying to Vertex AI Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c85ee62",
      "metadata": {
        "id": "7c85ee62"
      },
      "source": [
        "## Prerequisites\n",
        "1. A [Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench) notebook (or another environment with GCP SDK configured).\n",
        "2. A GCP project with Vertex AI and Artifact Registry enabled.\n",
        "3. Sufficient GPU quota if you want to do GPU-based fine-tuning.\n",
        "4. Python libraries: `transformers`, `torch`, `datasets`, and `google-cloud-storage`.\n",
        "\n",
        "## Steps\n",
        "1. **Install/Upgrade Libraries** (if needed)\n",
        "2. **Authenticate** to GCP\n",
        "3. **Set Project Variables**\n",
        "4. **Fine-tune** a GPT-like model (using `distilgpt2` as an example)\n",
        "5. **Export** and **upload** model artifacts to GCS\n",
        "6. **Build** a custom container image with a FastAPI inference service\n",
        "7. **Upload** the container to Artifact Registry\n",
        "8. **Create** Vertex AI Model & Endpoint\n",
        "9. **Deploy** model to the Endpoint\n",
        "10. **Test** the Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d4396b",
      "metadata": {
        "id": "29d4396b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers datasets torch fastapi uvicorn google-cloud-storage\n",
        "!pip install google-cloud-aiplatform --upgrade\n",
        "!pip install nest-asyncio  # Required if running FastAPI in the notebook for testing\n",
        "!pip install --upgrade \"accelerate>=0.26.0\" transformers[torch]\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "178eb224",
      "metadata": {
        "id": "178eb224",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check if gcloud is working:\n",
        "!gcloud --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "236c15a0",
      "metadata": {
        "id": "236c15a0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# TODO: Update these variables to match your GCP configuration.\n",
        "PROJECT_ID = \"personal-439503\"           # e.g., 'my-project-123'\n",
        "REGION = \"us-west1\"                      # or choose your preferred region\n",
        "BUCKET_NAME = \"llm-demo-bucket2\"          # e.g., 'my-vertex-bucket'\n",
        "REPO_NAME = \"llm-demo-repo\"                # Artifact Registry repository name\n",
        "IMAGE_NAME = \"llm-demo-image\"            # Docker image name\n",
        "IMAGE_TAG = \"v1\"                         # Docker image tag\n",
        "MODEL_DISPLAY_NAME = \"llm-demo-model\"    # Display name in Vertex AI\n",
        "ENDPOINT_DISPLAY_NAME = \"llm-demo-endpoint\"    # Display name in Vertex AI\n",
        "\n",
        "# Ensure your environment is set to the correct project:\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7677bccc",
      "metadata": {
        "id": "7677bccc"
      },
      "source": [
        "## 1. Fine-Tune the Model\n",
        "Here, we'll just do a very quick demonstration using `distilgpt2`. In reality, you'd have your own dataset and more robust training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbfef68",
      "metadata": {
        "id": "adbfef68",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Example: using distilgpt2 for a quick demonstration\n",
        "model_name = \"distilbert/distilgpt2\"\n",
        "# model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# ADD: configure tokenizer to use a padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# ADD: This data collator automatically sets labels = input_ids for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Important for GPT-style (causal) models\n",
        ")\n",
        "\n",
        "# Prepare a toy dataset\n",
        "# We'll just use the 'wikitext' dataset for demonstration, but you'll likely have your own.\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "def remove_empty(example):\n",
        "    return len(example[\"input_ids\"]) > 0\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
        "\n",
        "# ADD: filter out empty examples\n",
        "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].filter(remove_empty)\n",
        "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].filter(remove_empty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece9062a",
      "metadata": {
        "id": "ece9062a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./\" + MODEL_DISPLAY_NAME,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,  # For demo, just 1 epoch\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,\n",
        "    do_eval=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"].select(range(1000)),  # Take a subset for quick demo\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator # ADD: added\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca5f2f7b",
      "metadata": {
        "id": "ca5f2f7b"
      },
      "source": [
        "## 2. Save the Fine-Tuned Model\n",
        "We’ll save the model and tokenizer locally in the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7938a9e",
      "metadata": {
        "id": "f7938a9e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "save_path = \"./\" + MODEL_DISPLAY_NAME\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(\"Model and tokenizer saved to\", save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4185d5f7",
      "metadata": {
        "id": "4185d5f7"
      },
      "source": [
        "## 3. Upload Model Artifacts to GCS\n",
        "We’ll store our model in a GCS bucket so our container can load it in Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3b78e5",
      "metadata": {
        "id": "db3b78e5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "GCS_PATH = f\"gs://{BUCKET_NAME}\"\n",
        "print(f\"GCS path: {GCS_PATH}\")\n",
        "!gsutil cp -r ./llm-demo-model \"{GCS_PATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3796b27b",
      "metadata": {
        "id": "3796b27b"
      },
      "source": [
        "## 4. Create Inference Script\n",
        "We need a Python script (e.g. `inference.py`) that loads the model and serves predictions via FastAPI. We'll write it to the notebook filesystem so we can build a container with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae28b2b4-f741-4823-8e07-feb6e28d06c2",
      "metadata": {
        "tags": [],
        "id": "ae28b2b4-f741-4823-8e07-feb6e28d06c2"
      },
      "outputs": [],
      "source": [
        "print(f\"Bucket name: {BUCKET_NAME}\")\n",
        "print(f\"Model name: {MODEL_DISPLAY_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587c332f",
      "metadata": {
        "id": "587c332f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "inference_script = \"\"\"\\\n",
        "import os\n",
        "import logging\n",
        "from typing import List\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from google.cloud import storage\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Environment variable for GCS path\n",
        "LOCAL_MODEL_DIR=\"llm-demo-model\"\n",
        "\n",
        "# Set device to CPU explicitly\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# confirm loading the model works\n",
        "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)\n",
        "model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_DIR)\n",
        "model.eval()\n",
        "\n",
        "class InferenceInstance(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int = 50  # Default value if not provided\n",
        "\n",
        "class InferenceRequest(BaseModel):\n",
        "    instances: List[InferenceInstance]\n",
        "\n",
        "class PredictionOutput(BaseModel):\n",
        "    generated_text: str\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    predictions: List[PredictionOutput]\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict(request: InferenceRequest):\n",
        "    try:\n",
        "        print(f\"Received prediction request: {request.json()}\")\n",
        "        if not request.instances:\n",
        "            raise HTTPException(status_code=400, detail=\"No instances provided.\")\n",
        "\n",
        "        predictions = []\n",
        "        for instance in request.instances:\n",
        "            prompt = instance.prompt\n",
        "            max_tokens = instance.max_tokens\n",
        "\n",
        "            print(f\"Received prompt: {prompt} with max_tokens: {max_tokens}\")\n",
        "\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    temperature=0.8,\n",
        "                )\n",
        "\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            print(f\"Generated text: {generated_text}\")\n",
        "            predictions.append(PredictionOutput(generated_text=generated_text))\n",
        "\n",
        "        return PredictionResponse(predictions=predictions)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"inference.py\", \"w\") as f:\n",
        "    f.write(inference_script)\n",
        "\n",
        "print(\"inference.py created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f1a5f3",
      "metadata": {
        "id": "28f1a5f3"
      },
      "source": [
        "## 5. Create a Dockerfile\n",
        "We’ll build a Docker image that:\n",
        "1. Installs Python & dependencies\n",
        "2. Copies `inference.py`\n",
        "3. Runs our FastAPI server on port 8080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77515280",
      "metadata": {
        "id": "77515280",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dockerfile_contents = \"\"\"\\\n",
        "# Use a standard Python runtime as a parent image (CPU-only)\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set environment variables to prevent Python from writing pyc files and buffering stdout\n",
        "ENV PYTHONDONTWRITEBYTECODE=1\n",
        "ENV PYTHONUNBUFFERED=1\n",
        "\n",
        "# Prevent interactive prompts during package installation\n",
        "ENV DEBIAN_FRONTEND=noninteractive\n",
        "# Set the timezone to UTC (you can change this as needed)\n",
        "ENV TZ=UTC\n",
        "\n",
        "# Install system dependencies\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    curl \\\n",
        "    gnupg \\\n",
        "    apt-transport-https \\\n",
        "    ca-certificates \\\n",
        "    git \\\n",
        "    software-properties-common \\\n",
        "    tzdata \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Install Python 3.9's pip (already included in python:3.9-slim, but ensure it's up-to-date)\n",
        "RUN python -m pip install --upgrade pip\n",
        "\n",
        "# Install CPU-only PyTorch and other Python dependencies\n",
        "RUN pip install torch==2.0.1+cpu torchvision==0.15.2+cpu torchaudio==2.0.2+cpu --index-url https://download.pytorch.org/whl/cpu\n",
        "RUN pip install transformers fastapi uvicorn google-cloud-storage\n",
        "\n",
        "# Copy inference script\n",
        "WORKDIR /app\n",
        "COPY inference.py /app/inference.py\n",
        "COPY llm-demo-model /app/llm-demo-model\n",
        "\n",
        "# Expose port 8080\n",
        "EXPOSE 8080\n",
        "\n",
        "# Define the default command to run your application\n",
        "CMD [\"uvicorn\", \"inference:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_contents)\n",
        "\n",
        "print(\"Dockerfile created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21538cf-8529-4f2f-a3fc-80ed5b440c2a",
      "metadata": {
        "tags": [],
        "id": "f21538cf-8529-4f2f-a3fc-80ed5b440c2a"
      },
      "source": [
        "## 5.5. Test the inference script and Docker image\n",
        "\n",
        "Let's test this before deploying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4977ac90-964e-43d9-868c-0b0179b0150c",
      "metadata": {
        "tags": [],
        "id": "4977ac90-964e-43d9-868c-0b0179b0150c"
      },
      "outputs": [],
      "source": [
        "# test model loading\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_path = \"./llm-demo-model\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535dcb81-ce63-425c-9139-db66774bc10c",
      "metadata": {
        "tags": [],
        "id": "535dcb81-ce63-425c-9139-db66774bc10c"
      },
      "outputs": [],
      "source": [
        "# Build the Docker image (replace image name/tag as needed)\n",
        "#!docker build -t llm-demo-inference:latest .\n",
        "\n",
        "# Run the container in the background, mapping container port 8080 to host port 8081\n",
        "#!docker run -d -p 8081:8080 --name test-inference llm-demo-inference:latest\n",
        "\n",
        "# Test the /predict endpoint using curl (adjust JSON payload as needed)\n",
        "#!curl -X POST -H \"Content-Type: application/json\" -d '{\"instances\":[{\"prompt\":\"What is the capital of Oregon?\",\"max_tokens\":250}]}' http://localhost:8081/predict\n",
        "\n",
        "# (Optional) Check container logs if needed\n",
        "#!docker logs test-inference\n",
        "\n",
        "# # (Optional) Clean up: stop and remove the container\n",
        "!docker stop test-inference\n",
        "!docker rm test-inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e292f6c",
      "metadata": {
        "tags": [],
        "id": "3e292f6c"
      },
      "source": [
        "## 6. Build & Push Docker Image to Artifact Registry\n",
        "We'll use [Cloud Build](https://cloud.google.com/build) to build and push the image. Make sure you have an [Artifact Registry repository](https://cloud.google.com/artifact-registry/docs/create-repo) created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f72845d",
      "metadata": {
        "id": "0f72845d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "AR_REPO = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
        "\n",
        "print(\"Building and pushing image to:\", AR_REPO)\n",
        "!gcloud builds submit --tag {AR_REPO} ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b72c2eb",
      "metadata": {
        "id": "9b72c2eb"
      },
      "source": [
        "## 7. Create a Vertex AI Model Resource\n",
        "We register our container image with Vertex AI as a custom model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c663d74",
      "metadata": {
        "id": "8c663d74",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# enable permissions\n",
        "# gcloud artifacts repositories add-iam-policy-binding llm-demo-repo \\\n",
        "#   --project=\"personal-439503\" \\\n",
        "#   --location=\"us-west1\" \\\n",
        "#   --member=\"serviceAccount:service-228086471049@gcp-sa-aiplatform.iam.gserviceaccount.com\" \\\n",
        "#   --role=\"roles/artifactregistry.reader\"\n",
        "\n",
        "!gcloud ai models upload \\\n",
        "   --region={REGION} \\\n",
        "   --display-name={MODEL_DISPLAY_NAME} \\\n",
        "   --container-image-uri={AR_REPO} \\\n",
        "   --container-health-route=/health \\\n",
        "   --container-predict-route=/predict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412b3bc4",
      "metadata": {
        "id": "412b3bc4"
      },
      "source": [
        "From the output of the above command, note the **`modelId`** (MODEL_ID). We’ll use it in the next step. If you miss it, you can retrieve it via:\n",
        "```bash\n",
        "gcloud ai models list --region=us-central1\n",
        "```\n",
        "or from the **Vertex AI -> Models** page in the Console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5d60b9",
      "metadata": {
        "id": "7c5d60b9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO: Replace with your actual MODEL_ID from the previous step\n",
        "MODEL_ID = \"701791883730354176\"  # e.g. 123456789\n",
        "print(\"Using MODEL_ID:\", MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd29041",
      "metadata": {
        "id": "7cd29041"
      },
      "source": [
        "## 8. Create an Endpoint & Deploy the Model\n",
        "We create a Vertex AI Endpoint, then deploy our model to that Endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660ff030",
      "metadata": {
        "id": "660ff030",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!gcloud ai endpoints create --region={REGION} --display-name={ENDPOINT_DISPLAY_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2749fbf1",
      "metadata": {
        "id": "2749fbf1"
      },
      "source": [
        "Again, note the **`endpointId`** from the output, or retrieve it later via:\n",
        "```bash\n",
        "gcloud ai endpoints list --region=us-central1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c310c2",
      "metadata": {
        "id": "11c310c2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO: Replace with your actual ENDPOINT_ID from the previous command\n",
        "ENDPOINT_ID = \"8528678600193343488\"  # e.g. 987654321\n",
        "print(\"Using ENDPOINT_ID:\", ENDPOINT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c3859b8",
      "metadata": {
        "id": "7c3859b8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!gcloud ai endpoints deploy-model {ENDPOINT_ID} \\\n",
        "  --region={REGION} \\\n",
        "  --model={MODEL_ID} \\\n",
        "  --display-name=\"llm-demo-deployment\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43946841",
      "metadata": {
        "id": "43946841"
      },
      "source": [
        "## 9. Test the Endpoint\n",
        "Once deployment is successful, we can call the Endpoint using the Python Vertex AI SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48118424",
      "metadata": {
        "id": "48118424"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "endpoint = aiplatform.Endpoint(\n",
        "    endpoint_name=f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}\"\n",
        ")\n",
        "\n",
        "instances = [{\"prompt\": \"What's the capital of California?\",\"max_tokens\": 400}]\n",
        "\n",
        "response = endpoint.predict(instances)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f581f6b",
      "metadata": {
        "id": "6f581f6b"
      },
      "source": [
        "If everything worked correctly, your custom container should respond with generated text from your fine-tuned model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1b4d30",
      "metadata": {
        "id": "de1b4d30"
      },
      "source": [
        "## Clean Up (Optional)\n",
        "You might want to:\n",
        "1. Stop or delete your endpoint to save costs: `gcloud ai endpoints delete ENDPOINT_ID --region=REGION`\n",
        "2. Remove the model: `gcloud ai models delete MODEL_ID --region=REGION`\n",
        "3. Delete the GCS files if you no longer need them: `gsutil rm -r gs://YOUR_BUCKET/gpt4o-mini-finetuned`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad6951c6-542a-4e84-a48e-a9bdff3726f4",
      "metadata": {
        "id": "ad6951c6-542a-4e84-a48e-a9bdff3726f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "conda-env-pytorch-pytorch",
      "name": "workbench-notebooks.m126",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
    },
    "kernelspec": {
      "display_name": "PyTorch 1-13 (Local)",
      "language": "python",
      "name": "conda-env-pytorch-pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}