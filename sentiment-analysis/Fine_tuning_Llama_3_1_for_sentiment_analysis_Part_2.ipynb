{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Llama 3.1 for sentiment analysis: Part 2\n",
        "\n",
        "**Introduction**: Multiclass classification is a common use case for traditional ML models. However, now that open source foundation LLMs are so easily accessible, it's worth exploring how easily they can applied to this domain as well.\n",
        "\n",
        "This notebook is part 1 of a 2-part series exploring how easily Llama 3.1-8B-Instruct can be fine-tuned for three-part sentiment analysis (positive,\n",
        "negative, neutral) on the [Dynasent](https://paperswithcode.com/dataset/dynasent) dataset.\n",
        "\n",
        "**Findings**: The foundation model achieves 71% accuracy utilizing zero-shot prompting, and a fine-tuned version involving quantized LoRA adapters achieves 84% accuracy using 37,500 examples. A smaller dataset involving 3,750 examples achieved 81% accuracy.\n",
        "\n",
        "**Environment**: Nvidia A100 on Google Colab with 40GB of GPU RAM, and 80GB of CPU RAM. Training completed in less than 45 mins, demonstrating the feasibility of this approach. Google Drive was used for storing access tokens.\n",
        "\n",
        "**Summary of steps**:\n",
        "\n",
        "Notebook 1:\n",
        "  - Download the Dynasent dataset from Hugging Face.\n",
        "  - Explore, clean, and prepare two smaller datasets:\n",
        "    - A small dataset involving 3,750 examples.\n",
        "    - A medium dataset involving 37,500 examples.\n",
        "\n",
        "Notebook 2 (this notebook):\n",
        "  - Load dataset, model and tokenizer\n",
        "  - Execute test with foundation model\n",
        "  - Fine-tune QLoRA adapers\n",
        "  - Merge with foundation model\n",
        "  - Execute test with fine-tuned model\n",
        "  - Save model and upload to Hugging Face"
      ],
      "metadata": {
        "id": "xDB2tYCu3pG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: libraries, dependencies, configurations, helper functions"
      ],
      "metadata": {
        "id": "PmGy36Q3jHsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAcT2CGWEjG8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U transformers\n",
        "%pip install -U datasets\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import wandb\n",
        "import pprint\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          EarlyStoppingCallback,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "\n",
        "from datasets import (Dataset,\n",
        "                      DatasetDict,\n",
        "                      concatenate_datasets,\n",
        "                      load_dataset,\n",
        "                      load_from_disk)\n",
        "\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login as hf_login"
      ],
      "metadata": {
        "id": "Ws6qaWH_G76q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# connect to huggingface\n",
        "hf_auth_token = userdata.get('HF_TOKEN')\n",
        "hf_login(hf_auth_token)\n",
        "\n",
        "# connect to weights and biases\n",
        "wb_auth_token = userdata.get('WB_TOKEN')\n",
        "wandb.login(key=wb_auth_token)\n",
        "\n",
        "# initialize wandb.ai with this project\n",
        "run = wandb.init(\n",
        "  project='Fine-tune for sentiment analysis',\n",
        "  job_type=\"training\",\n",
        "  anonymous=\"allow\"\n",
        ")\n",
        "\n",
        "# base model config\n",
        "base_model_namespace = \"meta-llama\"\n",
        "base_model = \"Llama-3.1-8B-Instruct\"\n",
        "base_model_name = f\"{base_model_namespace}/{base_model}\"\n",
        "\n",
        "# base model cache config\n",
        "base_model_cache_base_directory = \"/content/drive/MyDrive/.model_cache\"\n",
        "base_model_provider = \"huggingface\"\n",
        "base_model_cache_directory = f\"{base_model_cache_base_directory}/{base_model_provider}/{base_model_name}\"\n",
        "\n",
        "# base dataset cache config\n",
        "base_dataset_cache_base_directory = \"/content/drive/MyDrive/.dataset_cache\"\n",
        "base_dataset_provider = \"huggingface\"\n",
        "base_dataset_namespace = \"dynabench\"\n",
        "base_dataset = \"dynasent\"\n",
        "base_dataset_name_part = \"dynabench.dynasent.r1.all\"\n",
        "base_dataset_name = f\"{base_dataset_namespace}/{base_dataset}\"\n",
        "base_dataset_cache_directory = f\"{base_dataset_cache_base_directory}/{base_dataset_provider}/{base_dataset_name}/{base_dataset_name_part}\"\n",
        "\n",
        "# project config\n",
        "custom_model_name = \"llama-3.1-8B-sentiment-analysis\"\n",
        "dataset_directory = \"/content/drive/MyDrive/.data/sentiment_analysis_37500\"   # comment to use the smaller dataset\n",
        "#dataset_directory = \"/content/drive/MyDrive/.data/sentiment_analysis_3750\"  # uncomment to use the smaller dataset\n",
        "model_directory =f\"{base_model_cache_base_directory}/model-llama-3.1-8b-instruct\"\n",
        "labels = ['positive','negative','neutral']\n",
        "\n",
        "# mixed precision datatypes\n",
        "torch_dtype = torch.float16\n",
        "use_fp16 = True\n",
        "use_bf16 = False"
      ],
      "metadata": {
        "id": "H431jTUlFeTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "\n",
        "def verify_distribution(df, stratify_col, split_name):\n",
        "  result = df.groupby(stratify_col).size().reset_index(name='count')\n",
        "  result['%'] = result['count'] / result['count'].sum() * 100\n",
        "  result = result.sort_values('%', ascending=False)\n",
        "  print(f\"{split_name} dataset, {len(df)} examples\\n {result}\\n\")\n",
        "\n",
        "def generate_prompt(example, includeLabel=False):\n",
        "\n",
        "  label = example['gold_label'] if includeLabel else \"\"\n",
        "\n",
        "  return f\"\"\"\n",
        "  Analyze the sentiment of the following text. Classify as positive, negative, or neutral.\n",
        "\n",
        "  text: {example['sentence']}\n",
        "  label: {label}\"\"\".strip()\n",
        "\n",
        "def generate_prompt_with_label(example):\n",
        "  return generate_prompt(example, True)\n",
        "\n",
        "def generate_prompt_without_label(example):\n",
        "  return generate_prompt(example, False)\n",
        "\n",
        "def load_base_model(cache_directory, model_name, quantization_config, torch_dtype):\n",
        "\n",
        "  model = None\n",
        "\n",
        "  # if not cached, download and save\n",
        "  if not os.path.exists(cache_directory):\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config=quantization_config,\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch_dtype)\n",
        "\n",
        "    # cache locally\n",
        "    model.save_pretrained(cache_directory)\n",
        "    print(f\"Downloaded model, saved in model cache: {cache_directory}\")\n",
        "\n",
        "  # if cached, load from cache\n",
        "  else:\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      cache_directory,\n",
        "      quantization_config=quantization_config,\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch_dtype)\n",
        "\n",
        "    print(f\"Loaded from model cache: {cache_directory}\")\n",
        "\n",
        "  return model\n",
        "\n",
        "def predict(test, model, tokenizer):\n",
        "  y_pred = []\n",
        "\n",
        "  for i in tqdm(range(len(test))):\n",
        "    prompt = test.iloc[i][\"prompt\"]\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=2, # handles sentiment labels\n",
        "                    temperature=0.1)\n",
        "\n",
        "    result = pipe(prompt)\n",
        "\n",
        "    answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
        "\n",
        "    # Determine the predicted category\n",
        "    for label in labels:\n",
        "      if label in answer.lower():\n",
        "        y_pred.append(label)\n",
        "        break\n",
        "    else:\n",
        "      y_pred.append(\"none\")\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "def evaluate(y_true, y_pred, labels):\n",
        "\n",
        "  mapping = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "  def map_func(x):\n",
        "    return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
        "\n",
        "  y_true_mapped = np.vectorize(map_func)(y_true)\n",
        "  y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
        "  print(f\"\\nAccuracy: {accuracy:.3f}\")\n",
        "\n",
        "  # Generate accuracy report\n",
        "  unique_label_names = set(y_true_mapped)  # Get unique labels\n",
        "\n",
        "  for label in unique_label_names:\n",
        "    label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
        "    label_y_true = [y_true_mapped[i] for i in label_indices]\n",
        "    label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
        "    label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "    print(f\"Accuracy for label {labels[label]}: {label_accuracy:.3f}\")\n",
        "\n",
        "  # Generate classification report\n",
        "  class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(class_report)\n",
        "\n",
        "  # Generate confusion matrix\n",
        "  conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
        "  print(\"\\nConfusion Matrix:\")\n",
        "  print(conf_matrix)\n",
        "\n",
        "def find_all_module_names(model):\n",
        "\n",
        "  cls = bnb.nn.Linear4bit\n",
        "  lora_module_names = set()\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, cls):\n",
        "      names = name.split('.')\n",
        "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "  if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
        "    lora_module_names.remove('lm_head')\n",
        "\n",
        "  return list(lora_module_names)"
      ],
      "metadata": {
        "id": "sxM7Mbt_EESN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and process dataset"
      ],
      "metadata": {
        "id": "dZ4I_H46jSMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "ds = load_from_disk(dataset_directory)\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "MOr5lNRVqNF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify balance\n",
        "verify_distribution(ds[\"train\"].to_pandas(), \"gold_label\", \"train\")\n",
        "verify_distribution(ds[\"validation\"].to_pandas(), \"gold_label\", \"validation\")\n",
        "verify_distribution(ds[\"test\"].to_pandas(), \"gold_label\", \"test\")"
      ],
      "metadata": {
        "id": "5OtSwS2Gub24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to dataframes\n",
        "X_train = ds[\"train\"].to_pandas()\n",
        "X_val = ds[\"validation\"].to_pandas()\n",
        "X_test = ds[\"test\"].to_pandas()\n",
        "\n",
        "# generate prompts for training and evaluation data\n",
        "X_train.loc[:,'prompt'] = X_train.apply(generate_prompt_with_label, axis=1)\n",
        "X_val.loc[:,'prompt'] = X_val.apply(generate_prompt_with_label, axis=1)\n",
        "\n",
        "# generate test prompts and extract true labels\n",
        "y_true = X_test.loc[:,'gold_label']\n",
        "X_test = pd.DataFrame(X_test.apply(generate_prompt_without_label, axis=1), columns=['prompt'])\n",
        "\n",
        "# convert to datasets\n",
        "train_dataset = Dataset.from_pandas(X_train)\n",
        "val_dataset = Dataset.from_pandas(X_val)\n",
        "test_dataset = Dataset.from_pandas(X_test)"
      ],
      "metadata": {
        "id": "m3M0TNSp7iIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(X_train.iloc[0]['prompt'])"
      ],
      "metadata": {
        "id": "Cni7Z1xE9Lqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(X_test.iloc[0]['prompt'])"
      ],
      "metadata": {
        "id": "uPA5nOgU9qrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model and tokenizer; test prior to fine-tuning"
      ],
      "metadata": {
        "id": "aqFLJvhJ-BIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and tokenizer. Load memory-optimized version of the base model where weigths stored in 4-bit quantized format, but computations in bfloat16 or float16, depending on the hardware. These optimizations will speed up training."
      ],
      "metadata": {
        "id": "dQ1nQqvO-HH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True, # 4-bit quantized version\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch_dtype,\n",
        "  bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# load model\n",
        "model = load_base_model(base_model_cache_directory, base_model_name, bnb_config, torch_dtype)\n",
        "model.config.pretraining_tp = 1\n",
        "model.config.use_cache = False\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "zKB3avnm-CcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a custom predict function, which will use the text generation pipeline to predict labels from the \"prompt\" column."
      ],
      "metadata": {
        "id": "CcSljgMC-59Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(X_test, model, tokenizer)"
      ],
      "metadata": {
        "id": "yVcngTYu-8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also create a custom evaluate function that will use the predicted labels and ground truth labels to calculate performance metrics as well as a confusion matrix."
      ],
      "metadata": {
        "id": "nz-zb2UI-owG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "id": "7LyVl9Sm-pff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune model"
      ],
      "metadata": {
        "id": "nUaYBhXZQZer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the trainable modules in the model. We'll use a custom function to extract the modules; then we'll apply them to the LoRA config."
      ],
      "metadata": {
        "id": "5SfymWiPQ3rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modules = find_all_module_names(model)\n",
        "print(modules)"
      ],
      "metadata": {
        "id": "MmE1uBhzQr_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: obtained best performance with these values (after originally trying r=16, lora_alpha=32, lora_dropout=0.1)\n",
        "peft_config = LoraConfig(\n",
        "  r=64,\n",
        "  lora_alpha=16,\n",
        "  lora_dropout=0,\n",
        "  bias=\"none\",\n",
        "  task_type=\"CAUSAL_LM\",\n",
        "  target_modules=modules,\n",
        ")\n",
        "\n",
        "early_stopping_cb = EarlyStoppingCallback(\n",
        "  early_stopping_patience=3,                # stop if no improvement after 2 evals\n",
        "  early_stopping_threshold=0.0              # minimum improvement threshold\n",
        ")\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "  output_dir=f\"{model_directory}\",          # directory to save and repository id\n",
        "\n",
        "  per_device_train_batch_size=16,           # batch size per device during training\n",
        "  per_device_eval_batch_size=16,\n",
        "  gradient_accumulation_steps=2,            # number of steps before performing a backward/update pass\n",
        "  gradient_checkpointing=True,\n",
        "\n",
        "  # training length\n",
        "  num_train_epochs=1,                       # number of training epochs\n",
        "\n",
        "  # learning rate\n",
        "  learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
        "\n",
        "  # regularization\n",
        "  weight_decay=0.01,\n",
        "  warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
        "\n",
        "  # early-stopping\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False,                  # we want the lowest validation loss\n",
        "\n",
        "  # logging and evaluation\n",
        "  eval_strategy=\"steps\",\n",
        "  eval_steps=100,\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=100,\n",
        "\n",
        "  # additional\n",
        "  optim=\"paged_adamw_32bit\",\n",
        "  fp16=use_fp16,                            # determined dynamically\n",
        "  bf16=use_bf16,                            # determined dynamically\n",
        "  max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
        "  max_steps=-1,\n",
        "  group_by_length=False,\n",
        "  lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
        "  report_to=\"wandb\",                        # report metrics to w&b\n",
        "  max_seq_length=512,                       # should be enough to handle full context window\n",
        "  dataset_text_field=\"prompt\",\n",
        "  packing=False,\n",
        "  dataset_kwargs={\n",
        "    \"add_special_tokens\": False,\n",
        "    \"append_concat_token\": False,\n",
        "  },\n",
        "  gradient_checkpointing_kwargs={\n",
        "    'use_reentrant': False\n",
        "  }\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model=model,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=val_dataset,\n",
        "  peft_config=peft_config,\n",
        "  processing_class=tokenizer,\n",
        "  args=training_arguments,\n",
        "  callbacks=[early_stopping_cb],\n",
        ")"
      ],
      "metadata": {
        "id": "onK-yIyQQ67s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_qX6HIbgSt8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "4tmh3MN7S44j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model and tokenizer\n",
        "trainer.save_model(model_directory)\n",
        "tokenizer.save_pretrained(model_directory)\n",
        "\n",
        "# push to huggingface\n",
        "trainer.model.push_to_hub(\n",
        "  custom_model_name,\n",
        "  repo_type=\"model\",\n",
        "  use_temp_dir=True,\n",
        "  token=hf_auth_token)"
      ],
      "metadata": {
        "id": "dtWDKje-0SU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the tuned model - is it better than the foundation model?\n",
        "y_pred = predict(X_test, model, tokenizer)\n",
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "id": "1pllcUXu0XWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the small dataset (3750 examples), achieved an overall 81% accuracy and 81% f1 (macro) score.\n",
        "\n",
        "```\n",
        "Accuracy: 0.813\n",
        "Accuracy for label positive: 0.840\n",
        "Accuracy for label negative: 0.904\n",
        "Accuracy for label neutral: 0.696\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    positive       0.84      0.84      0.84       125\n",
        "    negative       0.76      0.90      0.83       125\n",
        "     neutral       0.85      0.70      0.77       125\n",
        "\n",
        "    accuracy                           0.81       375\n",
        "   macro avg       0.82      0.81      0.81       375\n",
        "weighted avg       0.82      0.81      0.81       375\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[105  11   9]\n",
        " [  6 113   6]\n",
        " [ 14  24  87]]\n",
        "```\n",
        "\n",
        "However, with the medium dataset (37,500 examples), achieved an overall 84% accuracy and 84% f1 (macro) score.\n",
        "```\n",
        "\n",
        "Accuracy: 0.842\n",
        "Accuracy for label positive: 0.847\n",
        "Accuracy for label negative: 0.885\n",
        "Accuracy for label neutral: 0.794\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    positive       0.86      0.85      0.85      1250\n",
        "    negative       0.85      0.88      0.87      1250\n",
        "     neutral       0.81      0.79      0.80      1250\n",
        "\n",
        "    accuracy                           0.84      3750\n",
        "   macro avg       0.84      0.84      0.84      3750\n",
        "weighted avg       0.84      0.84      0.84      3750\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[1059   58  133]\n",
        " [  45 1106   99]\n",
        " [ 127  130  993]]\n",
        "```"
      ],
      "metadata": {
        "id": "sahLqUPtM2b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge and export fine-tuned model"
      ],
      "metadata": {
        "id": "By-FQKxahBTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, reload a fresh base model and tokenizer. We do not want to load the previous quantized version. Then merge the adapter with the base model, save, and upload to Hugging Face."
      ],
      "metadata": {
        "id": "Pqjx7mRrhgBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reload base model\n",
        "reloaded_model = AutoModelForCausalLM.from_pretrained(\n",
        "  base_model_name,\n",
        "  low_cpu_mem_usage=True,\n",
        "  return_dict=True,\n",
        "  torch_dtype=torch_dtype,\n",
        "  device_map='auto',\n",
        "  force_download=True # we want to force download so as not to use quantized version\n",
        ")\n",
        "\n",
        "# merge model\n",
        "merged_model = PeftModel.from_pretrained(reloaded_model, model_directory)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# save merged model\n",
        "merged_model.save_pretrained(model_directory)\n",
        "\n",
        "# push merged model to huggingface hub\n",
        "merged_model.push_to_hub(custom_model_name, use_temp_dir=True, token=hf_auth_token)"
      ],
      "metadata": {
        "id": "G8XUiPslhEK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}