{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Llama 3.1 for sentiment analysis: Part 1\n",
        "\n",
        "**Introduction**: Multiclass classification is a common use case for traditional ML models. However, now that open source foundation LLMs are so easily accessible, it's worth exploring how easily they can applied to this domain as well.\n",
        "\n",
        "This notebook is part 1 of a 2-part series exploring how easily Llama 3.1-8B-Instruct can be fine-tuned for three-part sentiment analysis (positive,\n",
        "negative, neutral) on the [Dynasent](https://paperswithcode.com/dataset/dynasent) dataset.\n",
        "\n",
        "**Findings**: The foundation model achieves 71% accuracy utilizing zero-shot prompting, and a fine-tuned version involving quantized LoRA adapters achieves 84% accuracy using 37,500 examples. A smaller dataset involving 3,750 examples achieved 81% accuracy.\n",
        "\n",
        "**Environment**: Nvidia A100 on Google Colab with 40GB of GPU RAM, and 80GB of CPU RAM. Training completed in less than 45 mins, demonstrating the feasibility of this approach. Google Drive was used for storing access tokens.\n",
        "\n",
        "**Summary of steps**:\n",
        "\n",
        "Notebook 1 (this notebook):\n",
        "  - Download the Dynasent dataset from Hugging Face.\n",
        "  - Explore, clean, and prepare two smaller datasets:\n",
        "    - A small dataset involving 3,750 examples.\n",
        "    - A medium dataset involving 37,500 examples.\n",
        "\n",
        "Notebook 2:\n",
        "  - Load dataset, model and tokenizer\n",
        "  - Execute test with foundation model\n",
        "  - Fine-tune QLoRA adapers\n",
        "  - Merge with foundation model\n",
        "  - Execute test with fine-tuned model\n",
        "  - Save model and upload to Hugging Face"
      ],
      "metadata": {
        "id": "wMftzrBt36lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: libraries, dependencies, configurations, helper functions"
      ],
      "metadata": {
        "id": "PmGy36Q3jHsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAcT2CGWEjG8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U transformers\n",
        "%pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import transformers\n",
        "\n",
        "from transformers import (AutoTokenizer)\n",
        "\n",
        "from datasets import (Dataset,\n",
        "                      DatasetDict,\n",
        "                      concatenate_datasets,\n",
        "                      load_dataset,\n",
        "                      load_from_disk)\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login as hf_login"
      ],
      "metadata": {
        "id": "Ws6qaWH_G76q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# connect to huggingface\n",
        "hf_auth_token = userdata.get('HF_TOKEN')\n",
        "hf_login(hf_auth_token)\n",
        "\n",
        "# base dataset cache config\n",
        "base_dataset_cache_base_directory = \"/content/drive/MyDrive/.dataset_cache\"\n",
        "base_dataset_provider = \"huggingface\"\n",
        "base_dataset_namespace = \"dynabench\"\n",
        "base_dataset = \"dynasent\"\n",
        "base_dataset_name_part = \"dynabench.dynasent.r1.all\"\n",
        "base_dataset_name = f\"{base_dataset_namespace}/{base_dataset}\"\n",
        "base_dataset_cache_directory = f\"{base_dataset_cache_base_directory}/{base_dataset_provider}/{base_dataset_name}/{base_dataset_name_part}\"\n",
        "\n",
        "# project config\n",
        "dataset_directory = \"/content/drive/MyDrive/.data\"\n",
        "labels = [\"positive\", \"negative\", \"neutral\"]"
      ],
      "metadata": {
        "id": "H431jTUlFeTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "\n",
        "def load_base_dataset(cache_directory, dataset_name, part_name=None, trust_remote_code=True):\n",
        "\n",
        "  dataset = None\n",
        "\n",
        "  # if not cached, download and save\n",
        "  if not os.path.exists(cache_directory):\n",
        "\n",
        "    if part_name is not None:\n",
        "      dataset = load_dataset(dataset_name, part_name, trust_remote_code=trust_remote_code)\n",
        "    else:\n",
        "      dataset = load_dataset(dataset_name, trust_remote_code=trust_remote_code)\n",
        "\n",
        "    # cache locally\n",
        "    os.makedirs(cache_directory)\n",
        "    dataset.save_to_disk(cache_directory)\n",
        "\n",
        "  # load from cache\n",
        "  else:\n",
        "    dataset = load_from_disk(cache_directory)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def split_label_subset(label_df, train_size, val_size, test_size, random_state):\n",
        "\n",
        "  label_df = label_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "  train_end = train_size\n",
        "  val_end  = train_size + val_size\n",
        "  test_end  = train_size + val_size + test_size\n",
        "\n",
        "  train_subset = label_df.iloc[:train_end]\n",
        "  val_subset  = label_df.iloc[train_end:val_end]\n",
        "  test_subset  = label_df.iloc[val_end:test_end]\n",
        "\n",
        "  return train_subset, val_subset, test_subset\n",
        "\n",
        "# note: dataset sizes applied per label\n",
        "def create_working_dataset(df, labels, train_size, val_size, test_size, random_state):\n",
        "\n",
        "  # shuffle once so sampling is random\n",
        "  df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "  # collect subsets per label\n",
        "  train_list, val_list, test_list = [], [], []\n",
        "  for label in labels:\n",
        "    label_df = df[df[\"gold_label\"] == label]\n",
        "    train_df, val_df, test_df = split_label_subset(label_df, train_size, val_size, test_size, random_state)\n",
        "    train_list.append(train_df)\n",
        "    val_list.append(val_df)\n",
        "    test_list.append(test_df)\n",
        "\n",
        "  # concatenate them back together, labeling each split\n",
        "  train_df = pd.concat(train_list, axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "  val_df  = pd.concat(val_list, axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "  test_df  = pd.concat(test_list, axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "  # combine again\n",
        "  train_df[\"split\"] = \"train\"\n",
        "  val_df[\"split\"]  = \"validation\"\n",
        "  test_df[\"split\"]  = \"test\"\n",
        "\n",
        "  final_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "  # validate\n",
        "  total_size = (train_size + val_size + test_size) * len(labels)\n",
        "  assert len(final_df) == total_size, \"Final dataset should be \" + total_size + \" examples total.\"\n",
        "\n",
        "  return train_df, val_df, test_df, final_df\n",
        "\n",
        "def save_working_dataset(train_df, val_df, test_df, final_df):\n",
        "\n",
        "  size = len(final_df)\n",
        "\n",
        "  # save the final balanced CSV\n",
        "  final_df.to_csv(f\"{dataset_directory}/sentiment_analysis_{size}.csv\", index=False)\n",
        "\n",
        "  # save huggingface version\n",
        "  hf_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df.drop(columns=[\"split\"])),\n",
        "    \"validation\": Dataset.from_pandas(val_df.drop(columns=[\"split\"])),\n",
        "    \"test\": Dataset.from_pandas(test_df.drop(columns=[\"split\"]))\n",
        "  })\n",
        "  hf_dataset.save_to_disk(f\"{dataset_directory}/sentiment_analysis_{size}\")\n",
        "\n",
        "def create_and_save_working_dataset(df, labels, train_size, val_size, test_size, random_state):\n",
        "\n",
        "  train_df, val_df, test_df, final_df = create_working_dataset(\n",
        "    df, labels, train_size, val_size, test_size, random_state)\n",
        "\n",
        "  save_working_dataset(train_df, val_df, test_df, final_df)"
      ],
      "metadata": {
        "id": "I3jhdInECP7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load, inspect, and save source dataset"
      ],
      "metadata": {
        "id": "dZ4I_H46jSMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_base_dataset(base_dataset_cache_directory, base_dataset_name, part_name=base_dataset_name_part)\n",
        "print(dataset)\n",
        "\n",
        "df = dataset['train'].to_pandas()\n",
        "df.info()"
      ],
      "metadata": {
        "id": "QX3z-aCCGabS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "iL01gpXWHlGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add % column\n",
        "result = df.groupby('gold_label').size().reset_index(name='count')\n",
        "result['%'] = result['count'] / result['count'].sum() * 100\n",
        "result = result.sort_values('%', ascending=False)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "xshphxT5ILSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare working dataset"
      ],
      "metadata": {
        "id": "vhM3ZcdyXOng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------\n",
        "# Convert to Pandas dataframe, clean, and save\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Combine all splits into one\n",
        "combined_dataset = concatenate_datasets([\n",
        "  dataset[\"train\"],\n",
        "  dataset[\"validation\"],\n",
        "  dataset[\"test\"]\n",
        "])\n",
        "\n",
        "# convert to pandas\n",
        "df = combined_dataset.to_pandas()\n",
        "\n",
        "# keep only the columns of interest\n",
        "df = df[[\"sentence\", \"gold_label\"]]\n",
        "\n",
        "# drop rows where these columns are null (NaN)\n",
        "df.dropna(subset=[\"sentence\", \"gold_label\"], inplace=True)\n",
        "\n",
        "# drop rows where labels are not the desired labels\n",
        "df = df[df[\"gold_label\"].isin(labels)]\n",
        "\n",
        "# drop rows where 'sentence' is an empty string (after stripping whitespace)\n",
        "df = df[df[\"sentence\"].str.strip() != \"\"]\n",
        "\n",
        "# reset index for cleanliness\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# save an intermediate CSV of the combined/cleaned data\n",
        "if not os.path.exists(dataset_directory):\n",
        "  os.makedirs(dataset_directory)\n",
        "df.to_csv(f\"{dataset_directory}/sentiment_analysis_clean.csv\", index=False)"
      ],
      "metadata": {
        "id": "D5BJJ0cbl6VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load cleaned\n",
        "df = pd.read_csv(f\"{dataset_directory}/sentiment_analysis_clean.csv\")\n",
        "\n",
        "# create two datasets - size: 3750, 37500\n",
        "create_and_save_working_dataset(df, labels, train_size=1000, val_size=125, test_size=125, random_state=42)\n",
        "create_and_save_working_dataset(df, labels, train_size=10000, val_size=1250, test_size=1250, random_state=42)"
      ],
      "metadata": {
        "id": "3LO8uiL15s_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate"
      ],
      "metadata": {
        "id": "QOTIGAZYjhDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_from_disk(f\"{dataset_directory}/sentiment_analysis_3750\")\n",
        "print(ds)\n",
        "\n",
        "ds = load_from_disk(f\"{dataset_directory}/sentiment_analysis_37500\")\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "MOr5lNRVqNF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgvefdKaenUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}